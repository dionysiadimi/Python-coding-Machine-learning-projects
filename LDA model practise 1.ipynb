{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTWs3GUaTqbuxKyruKLLXm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dionysiadimi/matzaris/blob/main/LDA%20model%20practise%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjKKWv9Fk01r",
        "outputId": "f1bdb6ab-acc1-47e8-8d97-be17d1695cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroup_train=fetch_20newsgroups(subset='train', shuffle=True)\n",
        "newsgroup_test=fetch_20newsgroups(subset='test',shuffle=True)\n",
        "\n",
        "print('done')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(newsgroup_train.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbZnP1zZrxn4",
        "outputId": "a1d14e5d-4d7b-4350-95fe-5db20228dfeb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(newsgroup_train.filenames.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YupjfLS-tUdy",
        "outputId": "3db28585-f53a-496a-ee57-c6cfb15ac0a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11314]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(newsgroup_test.filenames.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2h3d5Lztjti",
        "outputId": "99d31340-1383-4dfe-e979-6c5fb269aca0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7532]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHzLaNvatjv7",
        "outputId": "73ff41c3-7d5f-40b1-99a5-b22b3ad0fafc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random import seed\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(40)\n"
      ],
      "metadata": {
        "id": "kWH4M3J6tjy1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "s8eJx4JIw8k2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatize\n",
        "def lemmatize_stemming(text):\n",
        "  return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
      ],
      "metadata": {
        "id": "QSxwm5QAw8nW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize\n",
        "def preprocess(text):\n",
        "  result=[]\n",
        "\n",
        "  for token in gensim.utils.simple_preprocess(text):\n",
        "    if token not in gensim.parsing.preprocessing.STOPWORDS and len(token)>3:\n",
        "      result.append(lemmatize_stemming(token))\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "dz8dALoLz8V1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#make the list\n",
        "preprocessed_docs=[]\n",
        "for doc in newsgroup_train.data:\n",
        "  preprocessed_docs.append(preprocess(doc))"
      ],
      "metadata": {
        "id": "YXiuvypgz8Yj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(preprocessed_docs[:2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s0Q1LiPz8bX",
        "outputId": "1ad20139-0de1-4250-902a-a863528a569c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['lerxst', 'thing', 'subject', 'nntp', 'post', 'host', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', 'wonder', 'enlighten', 'door', 'sport', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'histori', 'info', 'funki', 'look', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst'], ['guykuo', 'carson', 'washington', 'subject', 'clock', 'poll', 'final', 'summari', 'final', 'clock', 'report', 'keyword', 'acceler', 'clock', 'upgrad', 'articl', 'shelley', 'qvfo', 'innc', 'organ', 'univers', 'washington', 'line', 'nntp', 'post', 'host', 'carson', 'washington', 'fair', 'number', 'brave', 'soul', 'upgrad', 'clock', 'oscil', 'share', 'experi', 'poll', 'send', 'brief', 'messag', 'detail', 'experi', 'procedur', 'speed', 'attain', 'rat', 'speed', 'card', 'adapt', 'heat', 'sink', 'hour', 'usag', 'floppi', 'disk', 'function', 'floppi', 'especi', 'request', 'summar', 'day', 'network', 'knowledg', 'base', 'clock', 'upgrad', 'haven', 'answer', 'poll', 'thank', 'guykuo', 'washington']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a dictionary\n",
        "dictionary=gensim.corpora.Dictionary(preprocessed_docs)\n",
        "len(dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A26zhuPV4ndm",
        "outputId": "5b05d17f-8a5f-4a63-b35b-6bff8601db98"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61411"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count=0\n",
        "for k,v in dictionary.iteritems():\n",
        "  print(k,v)\n",
        "  count +=1\n",
        "  if count >20:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b_NhwWD4ngr",
        "outputId": "586a07a9-1237-4441-98a4-10d312d1d1e9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 addit\n",
            "1 bodi\n",
            "2 bricklin\n",
            "3 bring\n",
            "4 bumper\n",
            "5 call\n",
            "6 colleg\n",
            "7 door\n",
            "8 earli\n",
            "9 engin\n",
            "10 enlighten\n",
            "11 funki\n",
            "12 histori\n",
            "13 host\n",
            "14 info\n",
            "15 know\n",
            "16 late\n",
            "17 lerxst\n",
            "18 line\n",
            "19 look\n",
            "20 mail\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filter this dictionary\n",
        "\n",
        "dictionary.filter_extremes(no_below=15,no_above=0.1)"
      ],
      "metadata": {
        "id": "OAnbityE4nji"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_corpus=[dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
        "document_num=50\n",
        "bow_doc_x=bow_corpus[document_num]\n",
        "for i in range(len(bow_doc_x)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0],\n",
        "                                                     dictionary[bow_doc_x[i][0]],\n",
        "                                                     bow_doc_x[i][1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfDYsWn64nmJ",
        "outputId": "13594e43-9487-4db1-a350-744bb2018d21"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word 4 (\"call\") appears 1 time.\n",
            "Word 48 (\"messag\") appears 1 time.\n",
            "Word 55 (\"request\") appears 5 time.\n",
            "Word 57 (\"share\") appears 4 time.\n",
            "Word 70 (\"advanc\") appears 1 time.\n",
            "Word 71 (\"anybodi\") appears 1 time.\n",
            "Word 80 (\"display\") appears 1 time.\n",
            "Word 127 (\"use\") appears 1 time.\n",
            "Word 165 (\"check\") appears 2 time.\n",
            "Word 172 (\"error\") appears 2 time.\n",
            "Word 179 (\"memori\") appears 3 time.\n",
            "Word 232 (\"john\") appears 2 time.\n",
            "Word 414 (\"run\") appears 1 time.\n",
            "Word 442 (\"creat\") appears 1 time.\n",
            "Word 466 (\"imag\") appears 1 time.\n",
            "Word 522 (\"collin\") appears 1 time.\n",
            "Word 541 (\"attach\") appears 1 time.\n",
            "Word 556 (\"current\") appears 1 time.\n",
            "Word 598 (\"major\") appears 1 time.\n",
            "Word 725 (\"minor\") appears 1 time.\n",
            "Word 774 (\"program\") appears 2 time.\n",
            "Word 801 (\"boston\") appears 1 time.\n",
            "Word 959 (\"output\") appears 1 time.\n",
            "Word 977 (\"fail\") appears 5 time.\n",
            "Word 983 (\"paramet\") appears 1 time.\n",
            "Word 1226 (\"serial\") appears 2 time.\n",
            "Word 1476 (\"have\") appears 1 time.\n",
            "Word 1534 (\"necessari\") appears 1 time.\n",
            "Word 1600 (\"crash\") appears 1 time.\n",
            "Word 1723 (\"alloc\") appears 1 time.\n",
            "Word 1724 (\"extens\") appears 1 time.\n",
            "Word 1725 (\"invalid\") appears 1 time.\n",
            "Word 1726 (\"openwindow\") appears 1 time.\n",
            "Word 1727 (\"process\") appears 1 time.\n",
            "Word 1728 (\"segment\") appears 4 time.\n",
            "Word 1729 (\"sparc\") appears 1 time.\n",
            "Word 1730 (\"stream\") appears 1 time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model=gensim.models.LdaMulticore(bow_corpus,\n",
        "                                   num_topics = 10,\n",
        "                                   id2word = dictionary,\n",
        "                                   passes = 10,\n",
        "                                   workers = 2)"
      ],
      "metadata": {
        "id": "xtDUjmJS4npE"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "  print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyFydDM9z8ga",
        "outputId": "4480d093-7839-40ef-826d-d1cdc221a499"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0 \n",
            "Words: 0.008*\"bike\" + 0.006*\"sale\" + 0.005*\"engin\" + 0.004*\"drive\" + 0.004*\"car\" + 0.004*\"leav\" + 0.003*\"motorcycl\" + 0.003*\"sell\" + 0.003*\"road\" + 0.003*\"ship\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.012*\"armenian\" + 0.010*\"israel\" + 0.008*\"isra\" + 0.007*\"kill\" + 0.006*\"turkish\" + 0.006*\"govern\" + 0.006*\"jew\" + 0.005*\"arab\" + 0.005*\"live\" + 0.004*\"attack\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.012*\"christian\" + 0.008*\"jesus\" + 0.006*\"exist\" + 0.005*\"moral\" + 0.005*\"bibl\" + 0.005*\"religion\" + 0.005*\"word\" + 0.005*\"church\" + 0.004*\"life\" + 0.004*\"claim\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.012*\"encrypt\" + 0.011*\"govern\" + 0.009*\"chip\" + 0.009*\"secur\" + 0.008*\"clipper\" + 0.007*\"presid\" + 0.006*\"public\" + 0.006*\"key\" + 0.005*\"clinton\" + 0.005*\"technolog\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.020*\"drive\" + 0.015*\"card\" + 0.010*\"scsi\" + 0.009*\"driver\" + 0.008*\"disk\" + 0.008*\"control\" + 0.007*\"video\" + 0.006*\"speed\" + 0.006*\"hard\" + 0.006*\"chip\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.018*\"game\" + 0.016*\"team\" + 0.012*\"play\" + 0.010*\"player\" + 0.007*\"hockey\" + 0.007*\"season\" + 0.005*\"leagu\" + 0.005*\"score\" + 0.004*\"basebal\" + 0.004*\"toronto\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.017*\"window\" + 0.015*\"file\" + 0.011*\"space\" + 0.010*\"program\" + 0.008*\"nasa\" + 0.008*\"imag\" + 0.006*\"avail\" + 0.006*\"version\" + 0.006*\"softwar\" + 0.005*\"server\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.005*\"gun\" + 0.005*\"control\" + 0.004*\"health\" + 0.004*\"firearm\" + 0.004*\"crime\" + 0.004*\"weapon\" + 0.004*\"report\" + 0.004*\"medic\" + 0.004*\"studi\" + 0.004*\"public\"\n",
            "\n",
            "\n",
            "Topic: 8 \n",
            "Words: 0.014*\"wire\" + 0.014*\"entri\" + 0.011*\"file\" + 0.010*\"program\" + 0.007*\"greek\" + 0.006*\"output\" + 0.006*\"grind\" + 0.006*\"section\" + 0.006*\"scienc\" + 0.006*\"book\"\n",
            "\n",
            "\n",
            "Topic: 9 \n",
            "Words: 0.009*\"price\" + 0.006*\"sell\" + 0.006*\"compani\" + 0.005*\"sale\" + 0.005*\"insur\" + 0.005*\"cost\" + 0.004*\"engin\" + 0.004*\"purdu\" + 0.004*\"servic\" + 0.004*\"printer\"\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test the model by looking the test set"
      ],
      "metadata": {
        "id": "N_DTYn8Nz8i7"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unseen_doc=newsgroup_test.data[100]\n",
        "print(unseen_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC50QHkyw8qK",
        "outputId": "cb8dc655-e966-4a86-8f6e-8e715c288862"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: help\n",
            "From: C..Doelle@p26.f3333.n106.z1.fidonet.org (C. Doelle)\n",
            "Lines: 13\n",
            "\n",
            "Hello All!\n",
            "\n",
            "    It is my understanding that all True-Type fonts in Windows are loaded in\n",
            "prior to starting Windows - this makes getting into Windows quite slow if you\n",
            "have hundreds of them as I do.  First off, am I correct in this thinking -\n",
            "secondly, if that is the case - can you get Windows to ignore them on boot and\n",
            "maybe make something like a PIF file to load them only when you enter the\n",
            "applications that need fonts?  Any ideas?\n",
            "\n",
            "\n",
            "Chris\n",
            "\n",
            " * Origin: chris.doelle.@f3333.n106.z1.fidonet.org (1:106/3333.26)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow_vector=dictionary.doc2bow(preprocess(unseen_doc))\n",
        "bow_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjne7Syrw8sv",
        "outputId": "f2f39676-bbdf-4a30-90e2-96b02401bc87"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(98, 1),\n",
              " (175, 1),\n",
              " (189, 1),\n",
              " (228, 1),\n",
              " (237, 1),\n",
              " (259, 1),\n",
              " (284, 1),\n",
              " (307, 1),\n",
              " (350, 1),\n",
              " (515, 1),\n",
              " (727, 1),\n",
              " (746, 1),\n",
              " (766, 2),\n",
              " (971, 1),\n",
              " (988, 4),\n",
              " (1025, 1),\n",
              " (1072, 2),\n",
              " (1075, 2),\n",
              " (1095, 1),\n",
              " (1951, 1),\n",
              " (3114, 1),\n",
              " (3462, 1),\n",
              " (3983, 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index,score in sorted(lda_model[bow_vector]):\n",
        "  print(\"Score {}\\t Topic :{} \".format(score,lda_model.print_topic(index,9)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXf3qoaqtj1H",
        "outputId": "e1c8ae44-7461-4f17-90a9-3599a6c52eee"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score 0.9709510803222656\t Topic :0.017*\"window\" + 0.015*\"file\" + 0.011*\"space\" + 0.010*\"program\" + 0.008*\"nasa\" + 0.008*\"imag\" + 0.006*\"avail\" + 0.006*\"version\" + 0.006*\"softwar\" \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Actual Topic',newsgroup_test.target[100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUq0THFstewm",
        "outputId": "814ab17a-1d3c-4e3c-d879-6c06793b3df6"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual Topic 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7u3kHvoExLG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}